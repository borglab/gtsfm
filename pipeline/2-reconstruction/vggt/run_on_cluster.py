"""
Run VGGT on clusters defined by a saved ClusterTree pickle.

This script mirrors the VGGT pipeline in
`pipeline/2-reconstruction/vggt/evaluation/test_co3d_cluster.py`, but it derives
cluster membership directly from a `cluster_tree.pkl` (generated by
`pipeline/1-partition/partition_metis_megaloc.py`). It writes COLMAP text outputs
under a GTSFM-style results tree, e.g. `<output_root>/results/C_1/.../vggt`.
"""

from __future__ import annotations

import argparse
import gc
import os
import pickle
import random
import shutil
import sys
from pathlib import Path
from typing import Iterable, Sequence

import hydra
import numpy as np
import torch
import torch.nn.functional as F
from hydra.utils import instantiate

from gtsfm.common.outputs import prepare_output_paths
from gtsfm.products.visibility_graph import visibility_graph_keys
from gtsfm.utils.tree import PreOrderIter, Tree

REPO_ROOT = Path(__file__).resolve().parents[3]
THIRDPARTY_VGGT_ROOT = REPO_ROOT / "thirdparty" / "vggt"
if THIRDPARTY_VGGT_ROOT.exists():
    sys.path.insert(0, str(THIRDPARTY_VGGT_ROOT))
THIRDPARTY_LIGHTGLUE_ROOT = REPO_ROOT / "thirdparty" / "LightGlue"
if THIRDPARTY_LIGHTGLUE_ROOT.exists():
    sys.path.insert(0, str(THIRDPARTY_LIGHTGLUE_ROOT))

from vggt.models.vggt import VGGT
from vggt.utils.geometry import unproject_depth_map_to_point_map
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.pose_enc import pose_encoding_to_extri_intri

_RESNET_MEAN = [0.485, 0.456, 0.406]
_RESNET_STD = [0.229, 0.224, 0.225]


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run VGGT on clusters from a saved ClusterTree.")
    parser.add_argument("--cluster_tree_path", type=str, required=True, help="Path to cluster_tree.pkl")
    parser.add_argument("--dataset_dir", type=str, required=True, help="Dataset root (used for loader).")
    parser.add_argument("--images_root", type=str, default=None, help="Root directory for images.")
    parser.add_argument(
        "--output_root",
        type=str,
        default="results/gerrard-hall/2-reconstruction/vggt_cluster_run",
        help="Base output directory (results will be in <output_root>/results/...).",
    )
    parser.add_argument(
        "--ba_output_root",
        type=str,
        default=None,
        help="Optional base output directory for BA results (defaults to output_root).",
    )
    parser.add_argument(
        "--config_name",
        type=str,
        default="vggt",
        help="Config in gtsfm/configs to load for the image loader.",
    )
    parser.add_argument(
        "--max_resolution",
        type=int,
        default=None,
        help="Optional loader max resolution override.",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default=str(THIRDPARTY_VGGT_ROOT / "weights" / "model.pt"),
        help="Path to the VGGT model checkpoint.",
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility.")
    parser.add_argument("--use_ba", action="store_true", default=False, help="Use BA for reconstruction.")
    parser.add_argument(
        "--ba_tracker",
        type=str,
        choices=["vggt", "vggsfm"],
        default="vggt",
        help="Tracker used for BA (vggt or vggsfm).",
    )
    parser.add_argument("--img_load_resolution", type=int, default=1024, help="Square load resolution for VGGT input.")
    parser.add_argument("--vggt_fixed_resolution", type=int, default=518, help="VGGT internal inference resolution.")
    parser.add_argument(
        "--tracking_max_query_pts",
        type=int,
        default=256,
        help="Max number of query points for VGGT tracking.",
    )
    parser.add_argument(
        "--tracking_query_frame_num",
        type=int,
        default=3,
        help="Number of query frames for VGGT tracking.",
    )
    parser.add_argument(
        "--tracking_keypoint_extractor",
        type=str,
        default="aliked",
        help="Keypoint extractor(s) for VGGT tracking (e.g. aliked, sp, sift, aliked+sp).",
    )
    parser.add_argument(
        "--conf_thres_value", type=float, default=5.0, help="Confidence threshold value for depth filtering."
    )
    parser.add_argument(
        "--save_tracking_outputs",
        action="store_true",
        default=False,
        help="Also save raw VGGT tracking outputs (pre-BA) in COLMAP text format.",
    )
    parser.add_argument("--run_leaf", action="store_true", default=True, help="Run VGGT on leaf clusters.")
    parser.add_argument("--run_parent", action="store_true", default=True, help="Run VGGT on non-leaf clusters.")
    parser.add_argument("--run_root", action="store_true", default=True, help="Run VGGT on the root cluster.")
    parser.add_argument("--min_images", type=int, default=2, help="Skip clusters with fewer images.")
    parser.add_argument(
        "--no_skip_existing",
        action="store_false",
        dest="skip_existing",
        default=True,
        help="Recompute even if output already exists.",
    )
    args = parser.parse_args()
    if args.images_root is None:
        args.images_root = os.path.join(args.dataset_dir, "images")
    if not (args.run_leaf or args.run_parent or args.run_root):
        args.run_leaf = True
    return args


def _build_loader(config_name: str, dataset_dir: str, images_dir: str | None, max_resolution: int | None):
    overrides: list[str] = [f"loader.dataset_dir={dataset_dir}"]
    if images_dir is not None:
        overrides.append(f"loader.images_dir={images_dir}")
    if max_resolution is not None:
        overrides.append(f"loader.max_resolution={max_resolution}")

    with hydra.initialize_config_module(config_module="gtsfm.configs", version_base=None):
        cfg = hydra.compose(config_name=config_name, overrides=overrides)
    return instantiate(cfg.loader)


def _load_cluster_tree(cluster_tree_path: str):
    with open(cluster_tree_path, "rb") as f:
        return pickle.load(f)


def _resolve_image_paths(image_names: Sequence[str], images_root: str | None) -> list[str]:
    resolved_paths = []
    for name in image_names:
        if os.path.isabs(name):
            resolved_paths.append(name)
        else:
            if images_root is None:
                raise ValueError("images_root is required when image filenames are relative.")
            resolved_paths.append(os.path.join(images_root, name))
    return resolved_paths


def _iter_clusters_with_paths(cluster_tree) -> Iterable[tuple[tuple[int, ...], Sequence[tuple[int, int]], bool]]:
    path_tree: Tree[tuple[tuple[int, ...], Sequence[tuple[int, int]]]] = cluster_tree.map_with_path(
        lambda path, visibility_graph: (path, visibility_graph)
    )
    for node in PreOrderIter(path_tree):
        path, visibility_graph = node.value
        yield path, visibility_graph, node.is_leaf()


def _should_run_cluster(path: tuple[int, ...], is_leaf: bool, args: argparse.Namespace) -> bool:
    if path == () and not args.run_root:
        return False
    if is_leaf and args.run_leaf:
        return True
    if (not is_leaf) and args.run_parent:
        return True
    return False


def _log_message(log_path: Path, message: str) -> None:
    with open(log_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"{message}\n")
    print(message)


def farthest_point_sampling(distance_matrix: torch.Tensor, num_samples: int, most_common_frame_index: int = 0) -> list[int]:
    distance_matrix = distance_matrix.clamp(min=0)
    num_frames = distance_matrix.size(0)
    selected_indices = [most_common_frame_index]
    check_distances = distance_matrix[selected_indices]

    while len(selected_indices) < num_samples:
        farthest_point = torch.argmax(check_distances)
        selected_indices.append(farthest_point.item())
        check_distances = distance_matrix[farthest_point]
        check_distances[selected_indices] = 0
        if len(selected_indices) == num_frames:
            break

    return selected_indices


def generate_rank_by_dino(
    images: torch.Tensor,
    query_frame_num: int,
    image_size: int = 518,
    model_name: str = "dinov2_vitb14_reg",
    device: str = "cuda",
    spatial_similarity: bool = True,
) -> list[int]:
    del image_size  # maintained for compatibility with original signature
    dino_v2_model = torch.hub.load("facebookresearch/dinov2", model_name)
    dino_v2_model.eval()
    dino_v2_model = dino_v2_model.to(device)

    resnet_mean = torch.tensor(_RESNET_MEAN, device=device).view(1, 3, 1, 1)
    resnet_std = torch.tensor(_RESNET_STD, device=device).view(1, 3, 1, 1)
    images_resnet_norm = (images - resnet_mean) / resnet_std

    with torch.no_grad():
        frame_feat = dino_v2_model(images_resnet_norm, is_training=True)

    if spatial_similarity:
        frame_feat = frame_feat["x_norm_patchtokens"]
        frame_feat_norm = F.normalize(frame_feat, p=2, dim=1)
        frame_feat_norm = frame_feat_norm.permute(1, 0, 2)
        similarity_matrix = torch.bmm(frame_feat_norm, frame_feat_norm.transpose(-1, -2)).mean(dim=0)
    else:
        frame_feat = frame_feat["x_norm_clstoken"]
        frame_feat_norm = F.normalize(frame_feat, p=2, dim=1)
        similarity_matrix = torch.mm(frame_feat_norm, frame_feat_norm.transpose(-1, -2))

    distance_matrix = 100 - similarity_matrix.clone()
    similarity_matrix.fill_diagonal_(-100)
    similarity_sum = similarity_matrix.sum(dim=1)
    most_common_frame_index = torch.argmax(similarity_sum).item()
    return farthest_point_sampling(distance_matrix, query_frame_num, most_common_frame_index)


def calculate_index_mappings(query_index: int, frame_num: int, device: str | torch.device | None = None) -> torch.Tensor:
    new_order = torch.arange(frame_num)
    new_order[0] = query_index
    new_order[query_index] = 0
    if device is not None:
        new_order = new_order.to(device)
    return new_order


def switch_tensor_order(tensors: list[torch.Tensor | None], order: torch.Tensor, dim: int = 1) -> list[torch.Tensor | None]:
    return [torch.index_select(tensor, dim, order) if tensor is not None else None for tensor in tensors]


def predict_track(
    model: VGGT,
    images: torch.Tensor,
    query_points: torch.Tensor,
    dtype: torch.dtype = torch.bfloat16,
    use_tf32_for_track: bool = True,
    iters: int = 4,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=dtype):
            images = images[None]
            aggregated_tokens_list, ps_idx = model.aggregator(images)
            if not use_tf32_for_track:
                track_list, vis_score, conf_score = model.track_head(
                    aggregated_tokens_list, images, ps_idx, query_points=query_points, iters=iters
                )

        if use_tf32_for_track:
            with torch.cuda.amp.autocast(enabled=False):
                track_list, vis_score, conf_score = model.track_head(
                    aggregated_tokens_list, images, ps_idx, query_points=query_points, iters=iters
                )

    pred_track = track_list[-1]
    return pred_track.squeeze(0), vis_score.squeeze(0), conf_score.squeeze(0)


def initialize_feature_extractors(
    max_query_num: int, det_thres: float, extractor_method: str = "aliked", device: str = "cuda"
) -> dict[str, torch.nn.Module]:
    from lightglue import ALIKED, SIFT, SuperPoint

    extractors = {}
    methods = extractor_method.lower().split("+")

    for method in methods:
        method = method.strip()
        if method == "aliked":
            extractors["aliked"] = ALIKED(max_num_keypoints=max_query_num, detection_threshold=det_thres).to(device).eval()
        elif method == "sp":
            extractors["sp"] = SuperPoint(max_num_keypoints=max_query_num, detection_threshold=det_thres).to(device).eval()
        elif method == "sift":
            extractors["sift"] = SIFT(max_num_keypoints=max_query_num).to(device).eval()
        else:
            print(f"Warning: Unknown feature extractor '{method}', ignoring.")

    if not extractors:
        print(f"Warning: No valid extractors found in '{extractor_method}'. Using ALIKED by default.")
        extractors["aliked"] = ALIKED(max_num_keypoints=max_query_num, detection_threshold=det_thres).to(device).eval()

    return extractors


def extract_keypoints(query_image: torch.Tensor, extractors: dict[str, torch.nn.Module], max_query_num: int) -> torch.Tensor:
    query_points_round = None
    with torch.no_grad():
        for extractor in extractors.values():
            query_points_data = extractor.extract(query_image)
            extractor_points = query_points_data["keypoints"].round()
            if query_points_round is not None:
                query_points_round = torch.cat([query_points_round, extractor_points], dim=1)
            else:
                query_points_round = extractor_points

    if query_points_round.shape[1] > max_query_num:
        random_point_indices = torch.randperm(query_points_round.shape[1])[:max_query_num]
        query_points_round = query_points_round[:, random_point_indices, :]
    return query_points_round


def run_vggt_tracking(
    model: VGGT,
    images: torch.Tensor,
    image_names: Sequence[str] | None = None,
    dtype: torch.dtype = torch.bfloat16,
    max_query_num: int = 2048,
    det_thres: float = 0.005,
    query_frame_num: int = 3,
    extractor_method: str = "aliked+sp+sift",
    camera_type: str = "SIMPLE_PINHOLE",
) -> dict:
    del image_names
    assert "RADIAL" not in camera_type, "RADIAL camera is not supported yet"

    device = images.device
    frame_num = images.shape[0]

    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=dtype):
            predictions = model(images)

    with torch.cuda.amp.autocast(dtype=torch.float64):
        extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions["pose_enc"], images.shape[-2:])
        pred_extrinsic = extrinsic[0]
        pred_intrinsic = intrinsic[0]
        depth_map, depth_conf = predictions["depth"][0], predictions["depth_conf"][0]
        world_points = unproject_depth_map_to_point_map(depth_map, pred_extrinsic, pred_intrinsic)

    query_frame_indexes = generate_rank_by_dino(
        images, query_frame_num, image_size=518, model_name="dinov2_vitb14_reg", device=device, spatial_similarity=False
    )
    if 0 in query_frame_indexes:
        query_frame_indexes.remove(0)
    query_frame_indexes = [0, *query_frame_indexes]

    world_points = torch.from_numpy(world_points).to(device)
    world_points_conf = depth_conf.to(device)
    torch.cuda.empty_cache()

    pred_tracks = []
    pred_vis_scores = []
    pred_conf_scores = []
    pred_world_points = []
    pred_world_points_conf = []

    extractors = initialize_feature_extractors(max_query_num, det_thres, extractor_method, str(device))

    for query_index in query_frame_indexes:
        query_image = images[query_index]
        query_points_round = extract_keypoints(query_image, extractors, max_query_num)

        reorder_index = calculate_index_mappings(query_index, frame_num, device=device)
        reorder_images = switch_tensor_order([images], reorder_index, dim=0)[0]
        reorder_tracks, reorder_vis_score, reorder_conf_score = predict_track(
            model, reorder_images, query_points_round, dtype=dtype, use_tf32_for_track=True, iters=4
        )
        pred_track, pred_vis, pred_score = switch_tensor_order(
            [reorder_tracks, reorder_vis_score, reorder_conf_score], reorder_index, dim=0
        )
        pred_tracks.append(pred_track)
        pred_vis_scores.append(pred_vis)
        pred_conf_scores.append(pred_score)

        query_points_round_long = query_points_round.squeeze(0).long()
        query_world_points = world_points[query_index][query_points_round_long[:, 1], query_points_round_long[:, 0]]
        query_world_points_conf = world_points_conf[query_index][query_points_round_long[:, 1], query_points_round_long[:, 0]]
        pred_world_points.append(query_world_points)
        pred_world_points_conf.append(query_world_points_conf)

    pred_tracks = torch.cat(pred_tracks, dim=1)
    pred_vis_scores = torch.cat(pred_vis_scores, dim=1)
    pred_conf_scores = torch.cat(pred_conf_scores, dim=1)
    pred_world_points = torch.cat(pred_world_points, dim=0)
    pred_world_points_conf = torch.cat(pred_world_points_conf, dim=0)

    filtered_flag = pred_world_points_conf > 1.5
    if filtered_flag.sum() > max_query_num // 2:
        pred_world_points = pred_world_points[filtered_flag]
        pred_world_points_conf = pred_world_points_conf[filtered_flag]
        pred_tracks = pred_tracks[:, filtered_flag]
        pred_vis_scores = pred_vis_scores[:, filtered_flag]
        pred_conf_scores = pred_conf_scores[:, filtered_flag]

    torch.cuda.empty_cache()
    _, _, H, W = images.shape
    image_size = torch.tensor([W, H], dtype=pred_tracks.dtype, device=device)
    masks = torch.logical_and(pred_vis_scores > 0.05, pred_conf_scores > 0.2)

    return {
        "pred_tracks": pred_tracks,
        "pred_vis_scores": pred_vis_scores,
        "pred_conf_scores": pred_conf_scores,
        "pred_world_points": pred_world_points,
        "pred_world_points_conf": pred_world_points_conf,
        "pred_extrinsic": pred_extrinsic,
        "pred_intrinsic": pred_intrinsic,
        "image_size": image_size,
        "masks": masks,
        "device": device,
        "camera_type": camera_type,
        "depth_map": depth_map,
        "depth_conf": depth_conf,
    }


def setup_model(args: argparse.Namespace):
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    model = VGGT()
    model.load_state_dict(torch.load(args.model_path))
    model.eval()
    model = model.to(device)
    return model, device, dtype


def _rotmat_to_colmap_qvec(R: np.ndarray) -> np.ndarray:
    Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat
    K = (
        np.array(
            [
                [Rxx - Ryy - Rzz, 0.0, 0.0, 0.0],
                [Ryx + Rxy, Ryy - Rxx - Rzz, 0.0, 0.0],
                [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0.0],
                [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz],
            ],
            dtype=np.float64,
        )
        / 3.0
    )
    eigvals, eigvecs = np.linalg.eigh(K)
    qvec = eigvecs[[3, 0, 1, 2], np.argmax(eigvals)]
    if qvec[0] < 0:
        qvec *= -1
    return qvec


def _camera_params_from_K(K: np.ndarray, camera_model: str) -> list[float]:
    fx = float(K[0, 0])
    fy = float(K[1, 1])
    cx = float(K[0, 2])
    cy = float(K[1, 2])
    if camera_model == "PINHOLE":
        return [fx, fy, cx, cy]
    if camera_model == "SIMPLE_PINHOLE":
        return [(fx + fy) * 0.5, cx, cy]
    raise ValueError(f"Unsupported camera model for export: {camera_model}")


def _project_points(
    points3d: np.ndarray, extrinsics: np.ndarray, intrinsics: np.ndarray, eps: float = 1e-8
) -> tuple[np.ndarray, np.ndarray]:
    points_h = np.concatenate([points3d, np.ones((points3d.shape[0], 1), dtype=points3d.dtype)], axis=1)
    cam_points = np.einsum("sij,nj->sni", extrinsics, points_h)
    z = cam_points[:, :, 2]
    x = cam_points[:, :, 0] / np.maximum(z, eps)
    y = cam_points[:, :, 1] / np.maximum(z, eps)
    ones = np.ones_like(x)
    normalized = np.stack([x, y, ones], axis=-1)
    pixels = np.einsum("sij,snj->sni", intrinsics, normalized)[:, :, :2]
    return pixels, z


def _export_tracking_to_colmap_text(
    tracking_outputs: dict,
    image_names: Sequence[str],
    out_dir: str | Path,
    images_tensor: torch.Tensor,
    shared_camera: bool = False,
    min_track_length: int = 2,
    max_reproj_error: float = 12.0,
) -> None:
    os.makedirs(out_dir, exist_ok=True)

    tracks = tracking_outputs["pred_tracks"].detach().cpu().numpy()
    masks = tracking_outputs["masks"].detach().cpu().numpy().astype(bool)
    points3d = tracking_outputs["pred_world_points"].detach().cpu().numpy()
    intrinsics = tracking_outputs["pred_intrinsic"].detach().cpu().numpy()
    extrinsics = tracking_outputs["pred_extrinsic"].detach().cpu().numpy()

    S, N, _ = tracks.shape
    if S != len(image_names):
        raise ValueError(f"Mismatch: {S} frames in tracking output but {len(image_names)} image names provided.")

    image_size = tracking_outputs["image_size"].detach().cpu().numpy()
    width = int(round(float(image_size[0])))
    height = int(round(float(image_size[1])))
    camera_model = tracking_outputs.get("camera_type", "SIMPLE_PINHOLE")
    if camera_model not in {"SIMPLE_PINHOLE", "PINHOLE"}:
        camera_model = "SIMPLE_PINHOLE"

    projected_xy, projected_z = _project_points(points3d, extrinsics, intrinsics)
    reproj_error = np.linalg.norm(projected_xy - tracks, axis=-1)
    reproj_mask = np.logical_and(projected_z > 0, reproj_error < max_reproj_error)
    valid_observations = np.logical_and(masks, reproj_mask)

    obs_counts = valid_observations.sum(axis=0)
    valid_point_mask = obs_counts >= min_track_length
    point_id_map = np.full(N, -1, dtype=np.int64)
    valid_point_indices = np.where(valid_point_mask)[0]
    for idx, point_idx in enumerate(valid_point_indices):
        point_id_map[point_idx] = idx + 1

    images_np = (images_tensor.detach().cpu().numpy() * 255.0).clip(0, 255).astype(np.uint8)
    images_np = images_np.transpose(0, 2, 3, 1)

    image_observations = [[] for _ in range(S)]
    point_tracks = {int(point_id_map[p]): [] for p in valid_point_indices}
    point_rgb = {}

    for frame_idx in range(S):
        visible_points = np.where(valid_observations[frame_idx])[0]
        for point_idx in visible_points:
            point3d_id = int(point_id_map[point_idx])
            if point3d_id < 0:
                continue
            x, y = tracks[frame_idx, point_idx]
            p2d_idx = len(image_observations[frame_idx])
            image_observations[frame_idx].append((float(x), float(y), point3d_id))
            point_tracks[point3d_id].append((frame_idx + 1, p2d_idx))
            if point3d_id not in point_rgb:
                u = int(np.clip(round(float(x)), 0, width - 1))
                v = int(np.clip(round(float(y)), 0, height - 1))
                rgb = images_np[frame_idx, v, u]
                point_rgb[point3d_id] = (int(rgb[0]), int(rgb[1]), int(rgb[2]))

    with open(os.path.join(out_dir, "cameras.txt"), "w", encoding="utf-8") as f:
        num_cameras = 1 if shared_camera else S
        f.write("# Camera list with one line of data per camera:\n")
        f.write("#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n")
        f.write(f"# Number of cameras: {num_cameras}\n")
        if shared_camera:
            params = _camera_params_from_K(intrinsics[0], camera_model)
            f.write(f"1 {camera_model} {width} {height} {' '.join(map(str, params))}\n")
        else:
            for frame_idx in range(S):
                params = _camera_params_from_K(intrinsics[frame_idx], camera_model)
                camera_id = frame_idx + 1
                f.write(f"{camera_id} {camera_model} {width} {height} {' '.join(map(str, params))}\n")

    with open(os.path.join(out_dir, "images.txt"), "w", encoding="utf-8") as f:
        f.write("# Image list with two lines of data per image:\n")
        f.write("#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n")
        f.write("#   POINTS2D[] as (X, Y, POINT3D_ID)\n")
        f.write(f"# Number of images: {S}\n")
        for frame_idx in range(S):
            image_id = frame_idx + 1
            camera_id = 1 if shared_camera else image_id
            R = extrinsics[frame_idx, :3, :3]
            t = extrinsics[frame_idx, :3, 3]
            qvec = _rotmat_to_colmap_qvec(R)
            name = image_names[frame_idx]
            f.write(
                f"{image_id} {qvec[0]} {qvec[1]} {qvec[2]} {qvec[3]} "
                f"{t[0]} {t[1]} {t[2]} {camera_id} {name}\n"
            )
            p2d_tokens = []
            for x, y, point3d_id in image_observations[frame_idx]:
                p2d_tokens.extend([str(x), str(y), str(point3d_id)])
            f.write((" ".join(p2d_tokens) + "\n") if p2d_tokens else "\n")

    with open(os.path.join(out_dir, "points3D.txt"), "w", encoding="utf-8") as f:
        f.write("# 3D point list with one line of data per point:\n")
        f.write("#   POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\n")
        f.write(f"# Number of points: {len(valid_point_indices)}\n")
        for point_idx in valid_point_indices:
            point3d_id = int(point_id_map[point_idx])
            xyz = points3d[point_idx]
            rgb = point_rgb.get(point3d_id, (255, 255, 255))
            track_tokens = []
            for image_id, p2d_idx in point_tracks[point3d_id]:
                track_tokens.extend([str(image_id), str(p2d_idx)])
            f.write(
                f"{point3d_id} {xyz[0]} {xyz[1]} {xyz[2]} {rgb[0]} {rgb[1]} {rgb[2]} 1.0 "
                + " ".join(track_tokens)
                + "\n"
            )


def run_vggt_reconstruction(
    args: argparse.Namespace,
    model: VGGT,
    device: str,
    dtype: torch.dtype,
    image_path_list: Sequence[str],
    output_dir: str,
    image_name_list: Sequence[str] | None = None,
) -> bool:
    if len(image_path_list) == 0:
        raise ValueError("No images provided to VGGT.")
    if image_name_list is None:
        image_name_list = [os.path.basename(path) for path in image_path_list]
    if next(model.parameters()).device.type != device:
        model.to(device)

    images = load_and_preprocess_images(image_path_list).to(device=device)
    tracking_outputs = run_vggt_tracking(
        model,
        images,
        image_names=list(image_name_list),
        dtype=dtype,
        max_query_num=args.tracking_max_query_pts,
        query_frame_num=args.tracking_query_frame_num,
        extractor_method=args.tracking_keypoint_extractor,
    )

    os.makedirs(output_dir, exist_ok=True)
    _export_tracking_to_colmap_text(
        tracking_outputs=tracking_outputs,
        image_names=image_name_list,
        out_dir=output_dir,
        images_tensor=images,
        shared_camera=False,
        min_track_length=2,
        max_reproj_error=12.0,
    )
    return True


def _promote_tracking_writer_outputs(output_dir: Path, log_path: Path) -> None:
    """Overwrite top-level COLMAP txt files with tracking_outputs writer results when available."""
    tracking_dir = output_dir / "tracking_outputs"
    required = ("cameras.txt", "images.txt", "points3D.txt")
    missing = [name for name in required if not (tracking_dir / name).exists()]
    if missing:
        _log_message(log_path, f"Tracking writer outputs missing in {tracking_dir}: {missing}. Keeping existing files.")
        return

    for name in required:
        shutil.copy2(tracking_dir / name, output_dir / name)
    _log_message(log_path, f"Promoted tracking writer outputs to {output_dir}.")


def _run_ba_on_saved_reconstruction(input_dir: Path, ba_output_dir: Path, log_path: Path, skip_existing: bool) -> bool:
    if skip_existing and (ba_output_dir / "cameras.txt").exists():
        _log_message(log_path, f"Skipping BA: output already exists at {ba_output_dir}.")
        return True

    required = ("cameras.txt", "images.txt", "points3D.txt")
    missing = [name for name in required if not (input_dir / name).exists()]
    if missing:
        _log_message(log_path, f"Skipping BA: missing {missing} in {input_dir}.")
        return False

    try:
        import pycolmap
    except ModuleNotFoundError as exc:
        raise ModuleNotFoundError("pycolmap is required for BA mode.") from exc

    reconstruction = pycolmap.Reconstruction(str(input_dir))
    if reconstruction.num_images() == 0:
        _log_message(log_path, f"Skipping BA: empty reconstruction at {input_dir}.")
        return False

    ba_options = pycolmap.BundleAdjustmentOptions()
    pycolmap.bundle_adjustment(reconstruction, ba_options)

    ba_output_dir.mkdir(parents=True, exist_ok=True)
    reconstruction.write_text(str(ba_output_dir))
    _log_message(log_path, f"Saved BA reconstruction to {ba_output_dir}.")
    return True


def _cleanup_after_cluster() -> None:
    gc.collect()
    try:
        import torch

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception:
        # Best-effort cleanup: skip torch-specific cleanup if unavailable.
        pass


def main() -> None:
    args = _parse_args()
    if not args.use_ba:
        # Force tracker execution for every non-BA cluster run.
        args.save_tracking_outputs = True

    cluster_tree = _load_cluster_tree(args.cluster_tree_path)
    if cluster_tree is None:
        raise ValueError(f"cluster_tree.pkl was empty or invalid: {args.cluster_tree_path}")

    image_names = image_paths = None
    if not args.use_ba:
        loader = _build_loader(args.config_name, args.dataset_dir, args.images_root, args.max_resolution)
        image_names = loader.image_filenames()
        images_root = args.images_root
        if images_root is None and hasattr(loader, "_images_dir"):
            images_root = getattr(loader, "_images_dir")
        image_paths = _resolve_image_paths(image_names, images_root)

    output_root = Path(args.output_root)
    ba_output_root = Path(args.ba_output_root) if args.use_ba and args.ba_output_root else output_root
    log_path = output_root / "vggt_cluster.log"
    output_root.mkdir(parents=True, exist_ok=True)

    model = device = dtype = None
    if not args.use_ba:
        model, device, dtype = setup_model(args)

    if args.use_ba:
        _log_message(
            log_path,
            f"BA mode: reading reconstructions from {output_root} and writing optimized models to {ba_output_root}.",
        )

    for path, visibility_graph, is_leaf in _iter_clusters_with_paths(cluster_tree):
        if not _should_run_cluster(path, is_leaf, args):
            continue

        image_indices = sorted(visibility_graph_keys(visibility_graph))
        if len(image_indices) < args.min_images:
            _log_message(log_path, f"Skipping {path}: only {len(image_indices)} images.")
            continue

        output_paths = prepare_output_paths(output_root, path)
        output_dir = output_paths.results / "vggt"
        if args.use_ba:
            ba_output_paths = prepare_output_paths(ba_output_root, path)
            ba_output_dir = ba_output_paths.results / "vggt"
        else:
            ba_output_dir = None

        try:
            if args.use_ba:
                _log_message(log_path, f"Running BA for {path}: {output_dir} -> {ba_output_dir}")
                _run_ba_on_saved_reconstruction(output_dir, ba_output_dir, log_path, args.skip_existing)
                continue

            if max(image_indices) >= len(image_names):
                _log_message(log_path, f"Skipping {path}: image index out of range (max={max(image_indices)}).")
                continue

            cluster_image_names = [image_names[idx] for idx in image_indices]
            cluster_image_paths = [image_paths[idx] for idx in image_indices]
            missing_paths = [p for p in cluster_image_paths if not os.path.exists(p)]
            if missing_paths:
                _log_message(log_path, f"Skipping {path}: missing {len(missing_paths)} images.")
                continue

            run_output_dir = output_dir
            if args.skip_existing and (run_output_dir / "cameras.txt").exists():
                _log_message(log_path, f"Skipping {path}: output already exists at {run_output_dir}.")
                continue

            _log_message(log_path, f"Running VGGT for {path} -> {run_output_dir}")
            run_vggt_reconstruction(
                args,
                model,
                device,
                dtype,
                cluster_image_paths,
                str(run_output_dir),
                image_name_list=cluster_image_names,
            )
            _promote_tracking_writer_outputs(run_output_dir, log_path)
        except Exception as exc:
            _log_message(log_path, f"Failed {path}: {exc!r}")
        finally:
            _cleanup_after_cluster()


if __name__ == "__main__":
    main()
